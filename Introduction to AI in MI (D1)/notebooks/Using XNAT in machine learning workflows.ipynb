{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b689ac",
   "metadata": {},
   "source": [
    "# Tutorial Day 1: Introduction to AI in medical imaging (part 2)\n",
    "\n",
    "During this session we will demonstrate how you can integrate XNAT and `xnatpy` into your research workflow. We will showcase this using a pretrained UNet model that was trained to segment cortical infarcts present on brain MRI scans. For our model we will use MONAI - a collection of libraries built to facilitate machine learning research in medical imaging. For more information about the model you can have a look at the original paper [Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation.](https://arxiv.org/abs/1505.04597) and an introduction to MONAI is available on the framework's [website](https://monai.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xnat\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import monai\n",
    "from monai.data import list_data_collate\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dce533",
   "metadata": {},
   "source": [
    "Download the model checkpoint shared with you by the winter school organizers and put it in the `'data'` folder. Now we will use built-in `UNet` class to create an instance of the model and load the weights of the pretrained model in the newly created instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = monai.networks.nets.UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(\n",
    "        torch.load(os.path.join(\"..\", \"data\", \"model.pth\"), map_location=torch.device('cpu')))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9145e2",
   "metadata": {},
   "source": [
    "Now, let's download some sample data that we will use to apply the network to. We have prepared two subjects (`Brain-0001` and `Brain-0002`) that have brain imaging data together with ground truth segmentations in NIfTI format. As we have discussed in the first part of the tutorial, you can associate any kind of data (and not just DICOM data) with an XNAT object. For this demo we have added the files to the subject's resources. Let's download and unzip data from one of the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67286d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xnat.connect('https://xnat.bmia.nl') as session:\n",
    "    project = session.projects['sandbox']\n",
    "    subj = project.subjects['Brain-0002']\n",
    "    subj.resources['nifti'].download(os.path.join(\"..\", \"data\", \"brain0002.zip\"))\n",
    "    with zipfile.ZipFile(os.path.join(\"..\", \"data\", \"brain0002.zip\"),\"r\") as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94fdc6",
   "metadata": {},
   "source": [
    "In order to construct a MONAI dataset that our network will be able to consume, we need to provide the paths to the data in the form of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b02_files = [{\"img\": os.path.join(\"..\", \"data\", \"Brain-0002/resources/nifti/files/image_flair_mask.nii.gz\"), \"seg\": os.path.join(\"..\", \"data\", \"Brain-0002/resources/nifti/files/image_seg.nii.gz\")}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0164153",
   "metadata": {},
   "source": [
    "We also need to instruct the dataset loader how to prepare data, which keys are associated with imaging data, etc. For this application the loader is quite minimal: we simply load the image and the corresponding segmentation, let the loader know which dimension is the channel dimension (so that subsequent transforms interpret it correctly), scale the image intensity, and finally, transform the data into PyTorch tensors. We also define some simple postprocessing (adding sigmoid activation operation to the model output and interpret the output of that as a discrete (0 or 1) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "            AsChannelFirstd(keys=[\"img\", \"seg\"], channel_dim=-1),\n",
    "            ScaleIntensityd(keys=\"img\"),\n",
    "            ToTensord(keys=[\"img\", \"seg\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "post_processing = Compose(\n",
    "    [Activations(sigmoid=True), AsDiscrete(threshold_values=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ab387",
   "metadata": {},
   "source": [
    "Finally, we can load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "b02_ds = monai.data.Dataset(data=b02_files, transform=transforms)\n",
    "loader = DataLoader(b02_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ece22",
   "metadata": {},
   "source": [
    "Since the model has been trained on a 96x96x96 windows, we need to apply `sliding_window_inference` method in order to get the prediction for the whole image. We also wrap it and make a higher-order function to simplify the subsequent usage (by omitting the parameters that are constant). In order to test the method performance outside of visual inspection, we will use Dice score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference method\n",
    "def inference(input):\n",
    "\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(96, 96, 96),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.25,\n",
    "        )\n",
    "    return _compute(input)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a28f86",
   "metadata": {},
   "source": [
    "Now, let's apply the model and visualize its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_tensor = b02_ds[0][\"img\"].unsqueeze(0).to(device)\n",
    "    segmentation_tensor = post_processing(inference(image_tensor))\n",
    "    true_segmentation_tensor = b02_ds[0][\"seg\"].unsqueeze(0).to(device)\n",
    "    \n",
    "    dice_score = dice_metric(y_pred=segmentation_tensor, y=true_segmentation_tensor)\n",
    "    \n",
    "    image = image_tensor.detach().cpu().numpy()\n",
    "    segmentation = segmentation_tensor.detach().cpu().numpy()\n",
    "    true_segmentation = true_segmentation_tensor.detach().cpu().numpy()\n",
    "\n",
    "print(f\"Dice score is: {dice_score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = 115\n",
    "\n",
    "f, axes = plt.subplots(1,3, figsize=(20, 15))\n",
    "axes[0].imshow(image[0,0,:,:,slc], cmap=\"gray\")\n",
    "axes[0].title.set_text(\"Original scan\")\n",
    "axes[1].imshow(true_segmentation[0,0,:,:,slc], cmap=\"gray\")\n",
    "axes[1].title.set_text(\"Ground truth\")\n",
    "axes[2].imshow(segmentation[0,0,:,:,slc], cmap=\"gray\")\n",
    "axes[2].title.set_text(\"Model output\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a9294",
   "metadata": {},
   "source": [
    "Try changing the slice index (`slc`) to see the results on different slices of the scan.\n",
    "## Exercise 1\n",
    "Apply the same model to the `Brain-0001` subject and investigate the output similarly to the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919f5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21aa8400",
   "metadata": {},
   "source": [
    "**(Optional)** If you have a local installation of XNAT you can upload the resulting segmentation to the corresponding subject using `resource.upload` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0caf7b283a0e57d26b0d61c5de33efa48e55f18ff9d7aa35555a093ab30c331c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
